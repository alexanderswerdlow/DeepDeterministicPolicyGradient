# Deep Deterministic Policy Gradient

The goal is to develop a policy that returns an action in some continuous space. As the name entails, this policy is deterministic, thus creating a mapping from state to action: <img alt="\pi^*(s)=a" src="https://render.githubusercontent.com/render/math?math=%5Cpi%5E%2a%28s%29%3Da" style="transform: translateY(20%);" />

We do this with an off-policy approach with an actor-critic structure. We create an actor <img alt="\pi_\theta(s)" src="https://render.githubusercontent.com/render/math?math=%5Cpi_%5Ctheta%28s%29" style="transform: translateY(20%);" /> that represents our policy and a critic <img alt="Q_\theta(s, a)" src="https://render.githubusercontent.com/render/math?math=Q_%5Ctheta%28s%2C%20a%29" style="transform: translateY(20%);" /> that learns the Q-function.

I chose an off-policy approach since I sought to apply this to control problems where we have no known starting expert trajectories and we may need significant exploration which is more difficult with on-policy approaches where the accuracy of <img alt="Q_\theta(s,a)" src="https://render.githubusercontent.com/render/math?math=Q_%5Ctheta%28s%2Ca%29" style="transform: translateY(20%);" /> is dependent on the current policy.

This off-policy approach uses "experience replay" by adding new transitions: <img alt="(s, a, r, s')" src="https://render.githubusercontent.com/render/math?math=%28s%2C%20a%2C%20r%2C%20s%27%29" style="transform: translateY(20%);" /> to a running dataset and recomputing the policy and Q-function gradient by selecting some batch from this dataset.

We use the following Q-value function definition seen in lecture:

<img alt="Q_\theta(s,a)\approx Q^*(s,a)=E[r(s,a) + \gamma\max_{a'} {Q^*(s', a')]}" src="https://render.githubusercontent.com/render/math?math=Q_%5Ctheta%28s%2Ca%29%5Capprox%20Q%5E%2a%28s%2Ca%29%3DE%5Br%28s%2Ca%29%20%2B%20%5Cgamma%5Cmax_%7Ba%27%7D%20%7BQ%5E%2a%28s%27%2C%20a%27%29%5D%7D" style="transform: translateY(20%);" /> where <img alt="s' \sim P(s'|s,a)" src="https://render.githubusercontent.com/render/math?math=s%27%20%5Csim%20P%28s%27%7Cs%2Ca%29" style="transform: translateY(20%);" />

In order to determine how our <img alt="Q_\theta(s,a)" src="https://render.githubusercontent.com/render/math?math=Q_%5Ctheta%28s%2Ca%29" style="transform: translateY(20%);" /> compares with the optimal <img alt="Q^*(s,a)" src="https://render.githubusercontent.com/render/math?math=Q%5E%2a%28s%2Ca%29" style="transform: translateY(20%);" />, we find the difference between <img alt="Q_\theta(s,a)" src="https://render.githubusercontent.com/render/math?math=Q_%5Ctheta%28s%2Ca%29" style="transform: translateY(20%);" /> and <img alt="r(s,a) + \gamma\max_{a'} {Q^*(s', a')}" src="https://render.githubusercontent.com/render/math?math=r%28s%2Ca%29%20%2B%20%5Cgamma%5Cmax_%7Ba%27%7D%20%7BQ%5E%2a%28s%27%2C%20a%27%29%7D" style="transform: translateY(20%);" />. However, since we don't know <img alt="a'" src="https://render.githubusercontent.com/render/math?math=a%27" style="transform: translateY(20%);" /> apriori, we use our actor <img alt="\pi_\theta(s)" src="https://render.githubusercontent.com/render/math?math=%5Cpi_%5Ctheta%28s%29" style="transform: translateY(20%);" /> to approximate this value. We construct our loss function as the expectation of MSE since we wish to compute the loss only over some random sample from our dataset:

<img alt="L(\theta)=E_{d\sim D}\bigg[\big(Q_\theta(s,a)-(r(s,a)+ \gamma Q_\theta(s', \pi_\theta(s')) \big)^2\bigg]" src="https://render.githubusercontent.com/render/math?math=L%28%5Ctheta%29%3DE_%7Bd%5Csim%20D%7D%5Cbigg%5B%5Cbig%28Q_%5Ctheta%28s%2Ca%29-%28r%28s%2Ca%29%2B%20%5Cgamma%20Q_%5Ctheta%28s%27%2C%20%5Cpi_%5Ctheta%28s%27%29%29%20%5Cbig%29%5E2%5Cbigg%5D" style="transform: translateY(20%);" /> where <img alt="d=(s,a,r,s')\in D" src="https://render.githubusercontent.com/render/math?math=d%3D%28s%2Ca%2Cr%2Cs%27%29%5Cin%20D" style="transform: translateY(20%);" />

We previously saw that we could find an optimal policy <img alt="\pi^*(s)" src="https://render.githubusercontent.com/render/math?math=%5Cpi%5E%2a%28s%29" style="transform: translateY(20%);" /> from the optimal Q-function <img alt="Q^*(s,a)" src="https://render.githubusercontent.com/render/math?math=Q%5E%2a%28s%2Ca%29" style="transform: translateY(20%);" /> and here our goal is the same. Thus we can define the gradient of our objective as

<p align="center"><img alt="
\grad L(\theta_\pi)=E_{d\sim D}\bigg[\grad_{\theta_\pi} Q_\theta(s,\pi_\theta(s))\bigg]=E_{d\sim D}\bigg[\grad_{\pi_\theta(s)} Q_\theta(s,\pi_\theta(s)) \grad_{\theta_\pi} \pi_{\theta_\pi}(s)\bigg]
" src="https://render.githubusercontent.com/render/math?math=%0A%5Cgrad%20L%28%5Ctheta_%5Cpi%29%3DE_%7Bd%5Csim%20D%7D%5Cbigg%5B%5Cgrad_%7B%5Ctheta_%5Cpi%7D%20Q_%5Ctheta%28s%2C%5Cpi_%5Ctheta%28s%29%29%5Cbigg%5D%3DE_%7Bd%5Csim%20D%7D%5Cbigg%5B%5Cgrad_%7B%5Cpi_%5Ctheta%28s%29%7D%20Q_%5Ctheta%28s%2C%5Cpi_%5Ctheta%28s%29%29%20%5Cgrad_%7B%5Ctheta_%5Cpi%7D%20%5Cpi_%7B%5Ctheta_%5Cpi%7D%28s%29%5Cbigg%5D%0A"/></p>

where <img alt="\theta_\pi" src="https://render.githubusercontent.com/render/math?math=%5Ctheta_%5Cpi" style="transform: translateY(20%);" /> denotes the function parameters of the actor specifically.

As discussed in lecture, stability of learning can be improved by computing the bellman value error using an actor and critic that lag behind the current actor and critic. Thus after each new transition we add to our dataset, we compute the aforementioned gradients, update our network parameters with some learning rate, and perform <img alt="\theta_{target} \leftarrow \tau \theta+(1-\tau) \theta_{target}" src="https://render.githubusercontent.com/render/math?math=%5Ctheta_%7Btarget%7D%20%5Cleftarrow%20%5Ctau%20%5Ctheta%2B%281-%5Ctau%29%20%5Ctheta_%7Btarget%7D" style="transform: translateY(20%);" /> where <img alt="\theta" src="https://render.githubusercontent.com/render/math?math=%5Ctheta" style="transform: translateY(20%);" /> parameterizes both the actor and critic.
